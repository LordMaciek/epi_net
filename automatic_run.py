from mesa import Agent, Model
from mesa.time import SimultaneousActivation
from mesa.space import NetworkGrid
from mesa.datacollection import DataCollector
from mesa.batchrunner import BatchRunner
import networkx as nx
import numpy as np
import random
import csv

HATER = 1
NO_HATER = 0

"""
The aim of this script is to generate a number of networks, following certain characteristics, and
running a simulation of hate speech outbreak, upon those networks. The network generating is dealt with
using the networkX library, whereas the interactions within the model - by MESA library.

Data will be collected in order to later determine, which network traits make the outbreak spread
quicker or slower.

The networks generated are according to following rules: 
Each Network is of same order N=1000
Each Network has similar (between 4 and 10) mean degree of nodes (((really? 4 and 10???)))
Each Network has to be connected

At this level, nets are generated by Barabasi-Albert algorithm in two versions:
- original
- with 'culling', by which I understand removing from the network those nodes,
which exceed a certain degree. At this moment, the threshold degree is set at 40, which
corresponds to 10% of the network's order. After removing, new nodes are added to compensate
for the drop in order.

The interactions between agents (represented by respectively, edges and nodes of our networks)
can be described as exposure to hate speech.
Each agent has his own tendency to utter hate speech. This can be understood as their contempt
towards a minority group, for example. The tendency corresponds to the probability of comitting
an act of hate speech by a given agent, and is of range (0,1). Each time an agent's neighbour
uses hate speech, this agent's probability of using HS - raises by a small number.
This represents the diminishing of social norms, which inhibit us from using HS.

And the last element - an agent must have used HS himself in the past, or have witnessed their
neighbour using it, in order to be able to use it themself. 
"""


# Define a function to read the network file with nets to be used in simulation.


def net_reader(file):
    with open(file) as f:
        feats = csv.reader(f)
        siatki = list(feats)
        return siatki


# Define a function for calculating the percentage of hating agents in each step.


def percent_haters(model):
    agent_behs = [agent.behavior for agent in model.schedule.agents]
    x = sum(agent_behs) / len(agent_behs)
    return x


# Define a function for calculating mean level of hate in the network at given step.


def average_hate(model):
    agent_hate = [agent.hate for agent in model.schedule.agents]
    x = sum(agent_hate) / len(agent_hate)
    return x


# Define a function that takes the parameters of newborn network and returns them


def net_parafara(model):
    x = model.parameters
    return x


net_setx = net_reader('nets_to_use.csv')[1:]
net_set = [tuple(i) for i in net_setx]

# Network generators

# def netgen_dba(n=1000, m1=5, m2=2, p=.64, cull=True, maxDeg=20):  # The default values, will not be used in the sim.
# get the global variable it to count attempts - can't be local because the function calls itself at failure.
def netgen_dba(n, m1, m2, p, cull, maxDeg):
    global attempt
    global usable_networks
    attempt += 1
    if attempt > 10:
        attempt = 0
        # When it exceeds 10, the function should stop the given run and return a code for failed attempt
        # at creating the net within assigned parameters.
        print(f"Failed generating network with parameters {m1, m2, p, cull, maxDeg}. Trying on.")
        return "fail"

    I = nx.dual_barabasi_albert_graph(n=n, m1=m1, m2=m2, p=p)  # Call the basic function
    degs = [I.degree[node] for node in list(I.nodes)]  # Calculate the node degree list for all nodes
    avg_deg = np.mean(degs)  # Calculate the mean node degree for the whole network.
    # conn = nx.average_node_connectivity(I)
    conn = 1
    if cull:  # Only if the network generator should remove supernodes.
        big_nodes = [node for node in list(I.nodes) if I.degree(node) >= maxDeg]  # Assigned SuperNode size.
        I.remove_nodes_from(big_nodes)
        for numerro in big_nodes:  # My very simple function for removing SuperNodes.
            I.add_node(numerro)
            eN = random.choice([m1, m2])
            for _ in range(eN):
                to = random.choice(list(I.nodes()))
                I.add_edge(numerro, to)
        degs = [I.degree[node] for node in list(I.nodes)]
        avg_deg = np.mean(degs)  # This whole loop should be a function, but method will not produce any new SuperNodes
        # conn = nx.average_node_connectivity(I)
        conn = 1
    if not (nx.is_connected(I) and (4 <= avg_deg <= 10)):  # Test if network within parameters
        return netgen_dba(n=n, m1=m1, m2=m2, p=p, maxDeg=maxDeg, cull=cull)  # If not within parameters, call self.
    else:
        attempt = 0  # Zero the attempt counter.
        usable_networks += 1
        print(f"Successfully generated naetwork with parameters {m1, m2, p, cull, maxDeg}\n"
              f"Number of usable networks = {usable_networks}.")
        return I
        # [m1, m2, p, cull, maxDeg, conn]  # Good idea?

attempt = 0
usable_networks = 0
networks_for_use = [netgen_dba(n=1000, m1=int(x[2]), m2=int(x[3]), p=float(x[4]),
                               cull=x[6], maxDeg=int(x[5])) for x in net_set[1:]]
# networks_for_use = map(netgen_dba(, net_set[1:10]) TODO Try to use map to map netgen_dba on the list of parameter sets.
print(len(networks_for_use))
networks_for_use = [x for x in networks_for_use if x != 'fail']
print(len(networks_for_use))
# Initialize global variable it, by seting it to zero


class NormAgent(Agent):
    def __init__(self, unique_id, model):
        super().__init__(unique_id, model)
        self.behavior = self.random.choices([NO_HATER, HATER], weights=[9, 1])[0]
        self.hate = self.random.betavariate(2, 5)
        self.knows_hatered = 0

    def step(self):
        neighbors_nodes = self.model.grid.get_neighbors(self.pos, include_center=False)
        neighbors = self.model.grid.get_cell_list_contents(neighbors_nodes)
        neigh_beh = [neigh.behavior for neigh in neighbors]

        self._nextBehavior = self.behavior
        self._nextHate = self.hate

        if HATER in [neigh_beh] or self.behavior == HATER:
            self.knows_hatered = 1

        if (self.hate > self.random.uniform(0, 1)) and (self.knows_hatered == 1):
            self._nextBehavior = HATER
        else:
            self._nextBehavior = NO_HATER

        if self.hate < 0.8:
            self._nextHate = self.hate + sum(neigh_beh) * 0.01

    def advance(self):
        self.hate = self._nextHate
        self.behavior = self._nextBehavior


class NormModel(Model):
    def __init__(self, size, set_no):
        self.num_agents = size
        self.num_nodes = self.num_agents
        self.set = set_no
        # self.set = net_set[set_no][2:-1]  # Read the net_set, remove old index data.
        # self.m1 = int(self.set[0])
        # self.m2 = int(self.set[1])
        # self.p = float(self.set[2])
        # self.cull = self.set[4]
        # self.maxDeg = int(self.set[3])
        # self.network = netgen_dba(1000, self.m1, self.m2, self.p, self.cull,
        #                           self.maxDeg)  # Returns a tuple (network, [list, of, parameters])
        # if self.network == "fail":
        #     self.running = False
        self.I = networks_for_use[self.set]
        # self.parameters = self.network[1]
        self.grid = NetworkGrid(self.I)
        self.schedule = SimultaneousActivation(self)
        self.running = True

        i = 0  # <- this shit necessary??
        list_of_random_nodes = self.random.sample(self.I.nodes(), self.num_agents)
        for i in range(self.num_agents):
            a = NormAgent(i, self)
            self.grid.place_agent(a, list_of_random_nodes[i])
            self.schedule.add(a)
            if percent_haters(self) > 0.8:  # When the percentage of haters in the model exceeds 80,
                self.running = False  # the simulation is stopped, data collected, and next one is started.

        self.datacollector = DataCollector(
            model_reporters={"PerHate": percent_haters,
                             "AveHate": average_hate,
                             },
            agent_reporters={"Hate": "behavior"}
        )

    def step(self):
        self.datacollector.collect(self)
        self.schedule.step()


# Fixed and set parameters for the simulation.
# As it's impossible to provide a preselected list of lists of values,
# only the ones most common in successfull net generations were passed.

fixed_params = {
    "size": 1000,
}

variable_params = {
    # "set_no": np.arange(len(net_set)),
    "set_no": np.arange(5),
}

# TODO Create a BatchRun with the right parameters

batch_run = BatchRunner(
    NormModel,
    variable_params,
    fixed_params,
    iterations=2,  # TODO How many iterations per set of parameters?
    max_steps=150,  # TODO How many steps per simulation run?

    # TODO Create a traditional model (no batchrun), based on this one. Run it and see how many runs we'll need.

    # TODO Decide, which network parameters and which agent paremeters are to be collected.
    # TODO Write reporters, which will collect the data in a usable fashion.
    # model_reporters={"PerHate": percent_haters,
    #                  "AveSens": average_sens,
    #                  "AveCont": average_contempt,
    #                  "CorHatCon": cor_hate_cont
    #                  },
    # agent_reporters={"Hate": "behavior",
    #                  "Sociability": "sociability",
    #                  "Step": "step_no"}
)

batch_run.run_all()

# TODO Save collected data to files for further analysis
# agent_data = batch_run.get_agent_vars_dataframe()
# run_data = batch_run.get_model_vars_dataframe()
# run_data.to_csv('zbiorcze.csv')
# agent_data.to_csv('agentami.csv')
